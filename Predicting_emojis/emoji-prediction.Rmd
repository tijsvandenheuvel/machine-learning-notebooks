---
title: "Emoji prediction"
author: "Tijs Van den Heuvel"
date: "10/15/2020"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
library(magrittr)
library(data.table)
```


# Tweet analysis and emoji prediction

The goal of this project is to predict an emoji based on text.

When looking for similar projects, they get to an accuracy of 33%, this will be the succes criteria

It would be great if an algorithm could suggest some emojis based on a different piece of text.

The data set consists of a collection of tweets with at least one emoji.

[data source](https://www.kaggle.com/rexhaif/emojifydata-en)

The data is gathered, selected and reformatted by _Daniil Larionov_ from the _ArchiveTeam Twitter data_ source

I transformed this data into a data table with the use of PySpark and regex in this jupyter notebook: 

[preprocess](tweet-pre-process.ipynb)

- I spend a lot of time making this fast enough for large data sets
- I did some initial experimentation and data cleaning with a 10 tweets set
- Then I created R code to construct a model with a 165 tweets set
- When that model worked (with an accuracy of 18%) I proceeded to fix the pre-processor to convert the entire data set

## load data 

### mini set

```{r load mini dataset, eval=FALSE}
tweetDT <- fread("./data/165tweets.csv")
```

### dev set (1.3 mil)

```{r load dev dataset, eval=FALSE}
tweetDT <- fread("./data/dev.csv")
```

### Full set

- already split into training and test set
- unpractical for cleansing and one hot encoding of output

```{r load full dataset, eval=TRUE}
tweetDT=rbind(fread("./data/train.csv"),fread("./data/test.csv"))
```

## a sample of the input data

```{r sample of raw input}
tweetDT[sample(1:.N,5)]
```

## Clean the tweet text

- to lower case
- keep only letters a-z
- (merge multiple spaces)
- (remove trailing spaces)
- remove empty tweets

> some of these steps could remove too much information, can be revised

```{r clean tweet text}
tweetDT$text<-tolower(tweetDT$text)
tweetDT$text<-gsub('[^a-z]+',' ',tweetDT$text)
#tweetDT$text<-gsub('\\s+',' ',tweetDT$text)
#tweetDT$text<-gsub(' $','',tweetDT$text)
tweetDT<- tweetDT[tweetDT$text!='',]
```

## a sample of the cleaned tweets

```{r sample of clean text}
tweetDT[sample(1:.N,5)]
``` 

## inspect tweets

```{r tweet length summary}
tweetDT$text %>% strsplit(" ") %>% sapply(length) %>% summary()
```

```{r tweet length plot}
tweetDT$text %>% strsplit(" ") %>% sapply(length) %>% hist(breaks=55,main="Distribution of words per tweet") 
```
### total vocabulary size

full 613994
lemmatized 529416

```{r how many different words}
tweetDT$text %>% strsplit(" ") %>% as.list() %>% unlist() %>% unique() %>%length()
```
### emoji count

this will be the size of the output layer of the network

```{r how many different emojis}
emoji_count <- unique(tweetDT$emoji)%>%length()
emoji_count
```

### how many tweets per emoji

```{r how many tweets per emoji}
emoji_freq<-as.data.frame(table(tweetDT$emoji))

top <- emoji_freq$Freq%>%order()%>%rev()%>%head(10)

emoji_freq[top,]
```

## Text classification with Tensorflow

I started from this tensorflow [tutorial](https://tensorflow.rstudio.com/tutorials/beginners/basic-ml/tutorial_basic_text_classification/)

- the input is the text of the tweet
- the output is the emoji
- a neural network takes in tensors and not text so I needed to prepare the data

```{r setup tensorflow message=FALSE}
library(keras)
library(dplyr)
library(ggplot2)
library(purrr)
```


### prepare the input data

- create a dictionary & represent each of the 10,000 most common words by an integer
- one hot encoding would create a 10,000D vector but that's too expensive
- Tokenization is also a method for encoding words with a vocabulary index based on word frequency
- text vectorization:
  - pad the arrays so they all have same length
  - create integer tensor of shape `num_words * max_length`
  - use an embedding layer capable of handling this shape as first layer
- `adapt` the text vectorization layer to learn about the data set vocabulary

```{r adapt text vectorization layer ,include=FALSE}
num_words <- 10000
max_length <- 20
text_vectorization <- layer_text_vectorization(
  max_tokens = num_words, 
  output_sequence_length = max_length, 
)
text_vectorization %>% adapt(tweetDT$text)
```

### compare max_length variations

- num words = 10000
- layer size = 192(input),192(dense),192
- 20 epochs

highscore 

- max_length=32
- 9.8 minutes / epoch
- accuracy 27.89

- max_length = 25
- 7.6 minutes / epoch
- accuracy 27.88 (27.91 on test)

- max_length = 20
- 6.3 minutes / epoch
- accuracy 27.82 (27.85 on test)

#### text vectorization insight

vocabulary sample

```{r the used vocabulary, eval=FALSE}
get_vocabulary(text_vectorization)%>%sample(100)
```

how text vectorization layer transforms input

```{r how the text vectorization transforms the input, eval=FALSE}
text_vectorization(matrix(tweetDT$text[7], ncol = 1))
```

### prepare the output data

- the emojis have to be encoded as a vector
- one hot is a decent method size there are 49 different emojis
- keras had built in function: to_categorical but it needs integer input
- convert labels to integers
- first column is entirely empty so I remove it
- create an emoji dictionary to find which number maps to which emoji after prediction

```{r vectorize emoji as one hot}
emoji_int <- match(tweetDT$emoji,unique(tweetDT$emoji))

one_hot_emoji <- to_categorical(emoji_int)
one_hot_emoji <- one_hot_emoji[,2:(emoji_count+1)]

emoji_dict <- tweetDT$emoji%>%unique()
```

### split the dataset into training & testing

- 80% training, 20% testing
- division arbitrarily chosen

```{r split data in test and training set}
training_id <- sample.int(nrow(tweetDT), size = nrow(tweetDT)*0.8)
training <- tweetDT[training_id,]
testing <- tweetDT[-training_id,]
oh_train <-one_hot_emoji[training_id,]
oh_test <- one_hot_emoji[-training_id,]

nrow(training)
nrow(testing)
```

### check effect of stemming

Stemming reduces the amount of different words.

This would mean that the information in the 10,000 words of the text vectorization layer is more dense.

Let's see if this has an impact on model performance

1. stem
2. text vectorization
3. split data 
4. train model
5. compare to score without stemming

- num words = 10000
- layer size = 192(input),192(dense),192

NO stemming

- 10.5 minutes / epoch
- overfits in +15 epochs
- accuracy 27.82%

WITH stemming

- 9.8 minutes / epoch
- overfits in 20 epochs
- accuracy 27.82% after 15, 27.89 after 20


```{r apply stemmer}
library(tm)

tweetDT$text <- stemDocument(tweetDT$text)
```

```{r sample stemmed text}
tweetDT[sample(1:.N,10)]$text
```

### design the model

- how many layers?
- how many hidden units?
- what types of building blocks?
- what type of architecture?

#### model 1 FFA

- I started from the model from the tutorial
- I added an extra dense layer of size 16
- I tried bigger layers of size 32 and 50
- I tried more layers, up to 5
- I removed most of the dropout because that made the accuracy increase
- I didn't realize that this was just overfitting
- Then I added more dropout after each layer and had an accuracy of 38%
- Then I realized I had duplicate rows because some tweets had multiples of the same emoji
- This caused the problem that the same record was in the training and test set
- When removing the duplicates, the accuracy dropped to 23%

initial FFA from tutorial:

```{r base FFA, eval = FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 16) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 16, activation = "relu") %>%
  layer_dropout(0.5) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

How many layers ? 1

tested with 50 unit layers

- 1 dense layer: 24.6% (19 epochs)
- 2 dense layers: 24.0%
- 3 dense layers: 23%
- 4 dense layers: 22%

how many units per layer?

- 40,50 units: 24.36%
- 40,45 units: 24.5%
- 50,50 units: 24.6%
- 55,50 units: 24.45%
- 55,55 units: 24.58%
- 60,55 units: 24.60%
- 70,60 units: 24.68%
- 80,70 units: 24.56%

```{r best FFA, eval=FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 70) %>%
  layer_global_average_pooling_1d() %>%
  layer_dense(units = 60, activation = "relu") %>%
  layer_dropout(0.4) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```


#### model 2 LSTM

LSTM's are a bit overkill since the sequences are a bit short: 32 words / tweet

GRU's outperform LSTM's on time and accuracy for small sequences

how many units per layer?

dev: 120s/epoch 10 epochs

- 50,50 units: 24.99% 
- 60,60 units: 25.15%
- 70,70 units: 24.97%

how many layers ?

tested with 60 units layers

- 1 layer: 25.15%
- 2 layers: 24.58%

```{r LSTM, eval=FALSE,warning=FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 60) %>%
  layer_lstm(units = 60,dropout=0.5, recurrent_dropout=0.5)%>%
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

#### model 3 GRU

similar too but less complex than LSTM which makes them twice as fast

1 layer 60,60: 24.88%
1 layer 70,60: 25.01%
1 layer 80,70: 25%
1 layer 90,70: 24.97%

amount of nodes doesn't seem to change all that much

dropout and recurrent dropout inside of the GRU layer makes it incompatible with the CUDNN library
which makes the computation a lot slower 

- 1 layer 
- 64,64 
- full set
- dropout 0.5 
- recurrent dropout 0.5
- 9 minutes / epoch
- accuracy: 26.15% (10 epochs)


```{r first GRU, eval=FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 64) %>%
  layer_gru(units = 64,dropout=0.5, recurrent_dropout=0.5)%>%
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

compare to cudnn without recurrent dropout
- 1 layer 
- 64,64 
- full set
- dropout 0.5 
- 6.5 minutes / epoch
- accuracy: 26.78% (11 epochs)

```{r best one layer GRU, eval=FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 64) %>%
  layer_gru(units = 64)%>%
  layer_dropout(0.5) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

- num words = 20000
- layer size = 64,64
- 8.5 minutes / epoch
- overfits in 7 epochs
- accuracy 26.75%

- num words = 20000
- layer size = 128,128
- 8.5 minutes / epoch
- overfits in 4 epochs
- accuracy 27.29%

- num words = 10000
- layer size = 128,128
- 7.8 minutes / epoch
- overfits in 6 epochs
- accuracy 27.25%

- num words = 10000
- layer size = 128,128,64
- 13.7 minutes / epoch
- overfits in 7 epochs
- accuracy 27.37%

- num words = 10000
- layer size = 128,192
- 8.6 minutes / epoch
- overfits in 5 epochs
- accuracy 27.46 %

- num words = 10000
- layer size = 192,192
- 9.3 minutes / epoch
- overfits  in 5 epochs
- accuracy 27.64 %

- num words = 10000
- layer size = 256,192
- 10 minutes / epoch
- overfits in 4 epochs
- accuracy 27.53 %

- num words = 10000
- layer size = 256,256
- 11 minutes / epoch
- overfits in 4 epochs
- accuracy 27.63 %

- num words = 10000
- layer size = 192,256
- 10.5  minutes / epoch
- overfits in 4 epochs
- accuracy 27.54 %

- num words = 10000
- layer size = 192,192,128(dense)
- 9.3 minutes / epoch
- overfits in 7 epochs
- accuracy 27.26 %

- num words = 10000
- layer size = 192(input),192(dense),192
- 10.5 minutes / epoch
- overfits in +15  epochs
- accuracy 27.82%


```{r best model, message=TRUE, warning=FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 192) %>%
  layer_dense(units=192)%>%
  layer_dropout(0.4)%>%
  layer_gru(units = 192)%>%
  layer_dropout(0.5) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```
 
#### model 4 bidirectional GRU

no improvement, much slower

- num words = 10000
- layer size = 128,128
- 14.3 minutes / epoch
- overfits in 5 epochs
- accuracy 27.27%

```{r bidirctional GRU, eval=FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 128) %>%
  bidirectional(layer_gru(units = 128))%>%
  layer_dropout(0.5) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

 
### compile the model 

the tutorial uses binary_crossentropy but our variables are categorical

```{r compile the model}
model %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = list('accuracy')
)
```

### Train the model

```{r start timer}
t1<-Sys.time()
```

```{r training}
history <- model %>% fit(
  training$text,
  oh_train,
  epochs = 20,
  batch_size = 1660, 
 #batch_size = 512, #cpu   
  validation_split = 0.2,
  verbose=2
) 
```

```{r stop timer}
time <- Sys.time()-t1
time
```

```{r plot training history, eval=FALSE}
plot(history)
```

### evaluate model

```{r evaluate model}
results <- model %>% evaluate(testing$text, oh_test, verbose = 0)
results
```

### single prediction

```{r single prediction}
id <- 9
amount <-10

prediction <- predict(model,testing$text[id])[1,]

top_val <- prediction[order(prediction)]%>%rev() %>%head(amount) %>% round(3)
top_ids <- order(prediction) %>% rev() %>% head(amount)

top_emoji <- emoji_dict[top_ids]
top_prediction <- cbind(top_emoji,top_val)

print(paste("text:",testing$text[id]))
print(paste("true:",testing$emoji[id]))
print(top_prediction)
```

### save model

```{r save model, eval = FALSE}
model %>% save_model_tf("my_model")
```

### load model

```{r load model, include=FALSE}
new_model <- load_model_tf("saved_model")
```



## accuracy distribution

of all predictions, which percentage is correct

add column of prediction to DT

```{r add column with emoji as int}
emoji_to_int<- function(emoji){
  return(which(emoji_dict==emoji))
}
emojint<- sapply(testing$emoji,emoji_to_int)
test_emojint<-cbind(testing,emojint)
```

10: 0.6s
100: 5s
1000: 44s
2000: 110s

```{r}
col<- predict(new_model,test_emojint$text)
```

```{r}
top_emoji <- apply(col,1,which.max)
```

```{r}
check_set = cbind(test_emojint$emojint,top_emoji)
```

```{r}

foo<- function(x){
  if(length(unique(x))==1){return(TRUE)}
  return(FALSE)
}

table(apply(check_set,1,foo))
```


```{r manual evaluation setup}

time1<-Sys.time()

make_pred <- function(dt){
  dt<- as.list(dt)
  id<-predict(new_model,dt$text)[1,]%>%which.max()
  if(dt$emojint==id){return(1)}
  return(0)
}

pred_set <- test_emojint[sample(1:.N,2000)]

pred1 <- apply(pred_set,1,make_pred)

time <- Sys.time()-time1
time
```

```{r manual evaluation}
perc<-Reduce("+",pred1)/length(pred1)
perc
```


## individual word emoji prediction

Make prediction for every word individually in text vectorization vocabulary.

Any prediction on one word gives the same emojis.

Here it is illustrated with five random words from voc, this is also true for words not in voc. 

The model has learned correlation between length and emoji.

```{r predict emoji from a single word,eval=false}
model_voc <- get_vocabulary(text_vectorization)

id<- c(1,50,100,3456,9999)

predict_pos <- function(id,pos) {
   pred<-predict(new_model,model_voc[id])[1,]%>% order()%>%rev()%>%head(pos)
   return(pred[pos])
}

print("first")
sapply(id,predict_pos,pos=1)
print("second")
sapply(id,predict_pos,pos=2)
print("third")
sapply(id,predict_pos,pos=3)
print("fourth")
sapply(id,predict_pos,pos=4)
print("fifth")
sapply(id,predict_pos,pos=5)

print("top predicted emoji by single word")
emoji_dict[sapply(c(1:5),predict_pos,id=1)]


```

## ngrams

```{r message=FALSE}
library(quanteda)
```

3gramming runs in 8 minutes

4gramming runs in 2 minutes

```{r}
t1<-Sys.time()

clean<-function(tweet){
  return(paste(tweet,collapse=' '))
}

gram<- tokens(tweetDT$text,"character") %>% tokens_ngrams(n=4,concatenator='')%>%lapply(clean)

tweetDT<- cbind(tweetDT,as.character(gram))

rm(gram)

Sys.time()-t1
```

```{r}
tweetDT[sample(1:.N,5)]
```

```{r tweet length summary}
tweetDT$V2 %>% strsplit(" ") %>% sapply(length) %>% summary()
```

```{r tweet length plot}
tweetDT$V2 %>% strsplit(" ") %>% sapply(length) %>% hist(breaks=162,main="Distribution of ngrams per tweet") 
```

there are 16625 different 3grams in the dev set
there are 229,941 different 4grams in the dev set


```{r how many different ngrams}
tweetDT$V2 %>% strsplit(" ") %>% as.list() %>% unlist() %>% unique() %>%length()
```

## machine learning

```{r setup tensorflow message=FALSE}
library(keras)
library(dplyr)
library(ggplot2)
library(purrr)
```


```{r how many different emojis}
emoji_count <- unique(tweetDT$emoji)%>%length()
emoji_count
```

```{r vectorize emoji as one hot}
one_hot_emoji <- match(tweetDT$emoji,unique(tweetDT$emoji))%>%to_categorical()
one_hot_emoji <- one_hot_emoji[,2:(emoji_count+1)]

emoji_dict <- tweetDT$emoji%>%unique()
```

```{r adapt text vectorization layer ,include=FALSE,cache=TRUE}
num_words <- 10000
max_length <- 100
text_vectorization <- layer_text_vectorization(
  max_tokens = num_words, 
  output_sequence_length = max_length
  )
text_vectorization %>% adapt(tweetDT$V2)
```

```{r the used vocabulary, eval=FALSE}
get_vocabulary(text_vectorization)%>%sample(100)
```

```{r split data in test and training set}
training_id <- sample.int(nrow(tweetDT), size = nrow(tweetDT)*0.8)
training <- tweetDT[training_id,]
testing <- tweetDT[-training_id,]
oh_train <-one_hot_emoji[training_id,]
oh_test <- one_hot_emoji[-training_id,]

nrow(training)
nrow(testing)
```

GPU memory cant handle bigger model nor the full dataset

- i: input
- d: dense
- g: gru

3-gram: 

- 100i,192d,192g (5 epochs) 24.35%
- 100i,222d,222g (380s x 6 epochs) 24.59%
- 100i,128d,128g,128g (560s x 10 epochs) 24.57%

4-gram:

- 100i,192d,192g (360s x 6) 24.64%

```{r best model, message=TRUE, warning=FALSE}
input <- layer_input(shape = c(1), dtype = "string")

output <- input %>% 
  text_vectorization() %>% 
  layer_embedding(input_dim = num_words + 1, output_dim = 192) %>%
  layer_dense(units=192)%>%
  layer_dropout(0.4)%>%
  layer_gru(units = 192)%>%
  layer_dropout(0.5) %>% 
  layer_dense(units = emoji_count, activation = "softmax")

model <- keras_model(input, output)
```

```{r compile the model}
model %>% compile(
  optimizer = 'adam',
  loss = 'categorical_crossentropy',
  metrics = list('accuracy')
)
```

```{r training}
history <- model %>% fit(
  training$V2,
  oh_train,
  epochs = 10,
  batch_size = 1660, 
 #batch_size = 512, #cpu   
  validation_split = 0.2,
  verbose=2
) 
```

```{r evaluate model}
results <- model %>% evaluate(testing$V2, oh_test, verbose = 0)
results
```




























